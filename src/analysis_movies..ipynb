{
  "metadata": {
    "name": "analysis_movies",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\nruntimeRDD \u003d movieDF.select(\"runtime\").filter(movieDF.runtime !\u003d \u0027\u0027).rdd\n\ndef isfloat(value):\n  try:\n    float(value)\n    return True\n  except ValueError:\n    return False\n\nruntime \u003d runtimeRDD.flatMap(lambda x: x).filter(lambda x: isfloat(x)).collect()\nfor i in range(len(runtime)):\n    runtime[i] \u003d float(runtime[i])\n\nbins \u003d [x for x in range(150)]\nplt.xlabel(\"time\")\nplt.ylabel(\"films\")\nplt.title(\"time for firm\")\nplt.hist(ratings, bins, color\u003d\"red\")\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\ndirectorRDD \u003d movieDF.select(\"director\").filter(movieDF.director !\u003d \u0027\u0027).rdd\ndirectorRDD \u003d directorRDD.flatMap(lambda x: x)\ndirectorPairRDD \u003d directorRDD.map(lambda x: (x, 1))\ndirectorPairRDD \u003d directorPairRDD.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending\u003dTrue)\n\ndirectorKeys \u003d directorPairRDD.keys().collect()\ndirectorValues \u003d directorPairRDD.values().collect()\n\nfigure(figsize\u003d(16, 9))\nplt.grid(axis\u003d\"x\")\nplt.xlabel(\"film\")\nplt.ylabel(\"director\")\nplt.title(\"top 10 director by number of film produced\")\n\nplt.barh(directorKeys[-10:], directorValues[-10:])\nplt.show()\n#plt.savefig(\"/datashared/figure1.png\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\ncertRDD \u003d movieDF.select(\"certificate\").filter(movieDF.certificate !\u003d \u0027\u0027).rdd\ncertPairRDD \u003d certRDD.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending\u003dFalse)\n\nallCertKeys \u003d certPairRDD.keys().collect()\nallCertValues \u003d certPairRDD.values().collect()\n\notherKey \u003d \"Other\"\notherValue \u003d 0\nfor i in allCertValues[6:]:\n    otherValue +\u003d i\n\ncertKeys \u003d allCertKeys[:6]\ncertKeys.append(otherKey)\n\ncertValues \u003d allCertValues[:6]\ncertValues.append(otherValue)\n\nplt.pie(certValues, labels \u003d certKeys, startangle\u003d90, autopct\u003d\u0027%1.1f%%\u0027)\nplt.show()\n# plt.savefig(\"/datashared/figure2.png\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\nratingRDD \u003d movieDF.select(\"rating\").filter(movieDF.rating !\u003d \u0027\u0027).rdd\n\ndef isfloat(value):\n  try:\n    float(value)\n    return True\n  except ValueError:\n    return False\n\nratings \u003d ratingRDD.flatMap(lambda x: x).filter(lambda x: isfloat(x)).collect()\nfor i in range(len(ratings)):\n    ratings[i] \u003d float(ratings[i])\n\nbins \u003d [x/10 for x in range(101)]\nplt.xlabel(\"points\")\nplt.ylabel(\"films\")\nplt.title(\"rating point spectrum\")\nplt.hist(ratings, bins, color\u003d\"g\")\nplt.show()\n# plt.savefig(\"/datashared/figure3.png\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\nratingRDD \u003d movieDF.select(\"metascore\").filter(movieDF.metascore !\u003d \u0027\u0027).rdd\n\ndef isfloat(value):\n  try:\n    float(value)\n    return True\n  except ValueError:\n    return False\n\nratings \u003d ratingRDD.flatMap(lambda x: x).filter(lambda x: isfloat(x)).collect()\nfor i in range(len(ratings)):\n    ratings[i] \u003d float(ratings[i])\n\nbins \u003d [x for x in range(101)]\nplt.xlabel(\"points\")\nplt.ylabel(\"films\")\nplt.title(\"metascore point spectrum\")\nplt.hist(ratings, bins, color\u003d\"orange\")\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\nyearRDD \u003d movieDF.select(\"year\").filter(movieDF.year !\u003d \u0027\u0027).rdd\nyearPairRDD \u003d yearRDD.flatMap(lambda x: x).filter(lambda x: x.isnumeric()).map(lambda x: (x, 1))\nyearPairRDD \u003d yearPairRDD.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[0], ascending\u003dTrue)\n\nyearKeys \u003d yearPairRDD.keys().collect()\nyearValues \u003d yearPairRDD.values().collect()\n\nfigure(figsize\u003d(16, 9))\nplt.xlabel(\"years\")\nplt.ylabel(\"films\")\nplt.title(\"film spectrum\")\nplt.grid(axis\u003d\"y\")\n\nplt.bar(yearKeys[-21:], yearValues[-21:], width\u003d0.5)\nplt.show()\n# plt.savefig(\"/datashared/figure4.png\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\nmPairRDD \u003d movieDF.select(\"rating\", \"metascore\").filter((movieDF.rating !\u003d \u0027\u0027) \u0026 (movieDF.metascore !\u003d \u0027\u0027)).rdd\n\nmRatings \u003d mPairRDD.keys().collect()\nmMetascores \u003d mPairRDD.values().collect()\n\nfor i in range(len(mRatings)):\n    mRatings[i] \u003d float(mRatings[i])\n    \nfor i in range(len(mMetascores)):\n    mMetascores[i] \u003d int(mMetascores[i])\n\nplt.xlabel(\"rating point\")\nplt.ylabel(\"metascore\")\nplt.title(\"rating-metascore scattering\")\n\nplt.scatter(mRatings, mMetascores, color\u003d\u0027green\u0027)\nplt.show()\n#plt.savefig(\"/datashared/figure5.png\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkConf\n\nconf \u003d SparkConf()\nconf.setMaster(\u0027spark://192.168.56.101:7077\u0027)\nconf.setAppName(\u0027Squad_Game\u0027)\n\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\n#sc \u003d SparkContext(conf\u003dconf)\nsc.setLogLevel(\"WARN\")\n\nspark \u003d SparkSession(sc)\n\nmovieDF \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").load(\"hdfs://hadoop-master:9000/dataset/dataset2/movies.csv\")\n\ngenreRDD \u003d movieDF.select(\"genre\").filter(movieDF.genre !\u003d \u0027\u0027).rdd\ntotalFilm \u003d genreRDD.count()\ngenrePairRDD \u003d genreRDD.flatMap(lambda x: x).flatMap(lambda x: x.split(\",\")).map(lambda x: (x, 1))\ngenrePairRDD \u003d genrePairRDD.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending\u003dFalse)\n\nallGenreKeys \u003d genrePairRDD.keys().collect()\nallGenreValues \u003d genrePairRDD.values().collect()\n\notherKey \u003d \"Other\"\notherValue \u003d 0\nfor i in allGenreValues[8:]:\n    otherValue +\u003d i\n\ngenreValues \u003d allGenreValues[:8]\ngenreValues.append(otherValue)\n\ngenreKeys \u003d allGenreKeys[:8]\ngenreKeys.append(otherKey)\nfor i in range(len(genreKeys)):\n    genreKeys[i] \u003d str(genreValues[i]) + str(\" films \") + genreKeys[i]\n\nfig, ax \u003d plt.subplots(figsize\u003d(16, 9), subplot_kw\u003ddict(aspect\u003d\"equal\"))\n\nwedges, texts \u003d ax.pie(genreValues, wedgeprops\u003ddict(width\u003d0.5), startangle\u003d90)\n\nbbox_props \u003d dict(boxstyle\u003d\"square,pad\u003d0.3\", fc\u003d\"w\", ec\u003d\"k\", lw\u003d0.72)\nkw \u003d dict(arrowprops\u003ddict(arrowstyle\u003d\"-\"),\n          bbox\u003dbbox_props, zorder\u003d0, va\u003d\"center\")\n\nfor i, p in enumerate(wedges):\n    ang \u003d (p.theta2 - p.theta1)/2. + p.theta1\n    y \u003d np.sin(np.deg2rad(ang))\n    x \u003d np.cos(np.deg2rad(ang))\n    horizontalalignment \u003d {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle \u003d \"angle,angleA\u003d0,angleB\u003d{}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(genreKeys[i], xy\u003d(x, y), xytext\u003d(1.35*np.sign(x), 1.4*y), horizontalalignment\u003dhorizontalalignment, **kw)\n\nax.set_title(\"Genres of \" + str(totalFilm)  + \" films\")\n\nplt.show()\n# plt.savefig(\"/datashared/figure6.png\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}